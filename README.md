# TwitterTweetsMoney
Textual information has opened new doors for understanding how information can persuade human behavior. In terms of political representation, members of both the House of Representatives and Senate often use Twitter as a means of communicating with their constituents, political party, and donors. The latter was investigated in this project. The main goal was to determine whether or not tweet sentiment and diversity could be used to better determine how much a senatorial candidate would be able to raise in campaign funds. 
In order to achieve this, information was scraped from several trusted online sources, including BallotPedia and Twitter. The basic information included state represented, political party, years served, and total funds raised in the previous election cycle, number of unique words in the past 1500 tweets, and the overall sentiment score of the past 1500 tweets. The 1500 number was chosen due to computational time and topics covered in congress over time. A particular time range was not chosen because there are several compounding factors, such as the senate race currently taking place in Alabama, outside of election year. 
Furthermore, senators were chosen instead of house representatives because each state is represented equally in the senate. Each state places two senators, so the funds should be less dependent on constituent demographics. For example, in representative races for the House of Representatives there have been many allegations of gerrymandering in order for special interest parties to win, but with senatorial races the votes do not depend on geographic location within a state. There was hope in finding a more representative (pun intended) sample using senators over house representatives.  
Once the section of Congress was chosen for investigation and the data was web scraped and pulled from Twitterâ€™s API a basic linear regression was chosen for numerical prediction. The prediction was simply funds raised (in dollars), based on two separate sets of predictor variables. One set including textual information and one set only using basic senator information. Mean Squared Error (MSE) and adjusted R2 values were chosen to determine whether or not the model with textual information performed better. Overall, the model including textual information performed better than the baseline, with a MSE decrease of 30% and an adjusted R2 increase of 19%. 
